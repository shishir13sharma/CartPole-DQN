{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer, PrioritizedReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        \n",
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(4, 16);\n",
    "        self.l2 = nn.Linear(16, 64);        \n",
    "        self.l3 = nn.Linear(64, 256);\n",
    "        self.l4 = nn.Linear(256, 2);\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))                \n",
    "        x = F.relu(self.l4(x))                \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN_learn():\n",
    "    def __init__(self, model, learning_rate, epsilon_sched, gamma, batch_size, act_space):\n",
    "        self.dqn = model\n",
    "        self.target_dqn = deepcopy(model)        \n",
    "        self.optim = optim.Adam(lr= learning_rate, params= self.dqn.parameters())\n",
    "        self.epsilon = epsilon_sched\n",
    "        self.batch_size = batch_size\n",
    "        self.act_space = act_space\n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer(50000)\n",
    "        self.update_freq = 100\n",
    "        self.train_freq = 1\n",
    "        self.t = 1\n",
    "        self.obs_t = np.array([])\n",
    "        \n",
    "    def train(self):        \n",
    "        \n",
    "        buffer_sample = self.replay_buffer.sample(self.batch_size)\n",
    "        obs = torch.from_numpy(buffer_sample[0]).float()\n",
    "        act = torch.from_numpy(buffer_sample[1]).long()\n",
    "        obs1 = torch.from_numpy(buffer_sample[2]).float()\n",
    "        rew = torch.from_numpy(buffer_sample[3]).float()\n",
    "        dones = torch.from_numpy(buffer_sample[4].astype(int)).float()                 \n",
    "                      \n",
    "        val = self.dqn(obs)\n",
    "        val = val.gather(1, act.view(-1, 1)).squeeze() # Q-values of chosen actions\n",
    "        \n",
    "        _, max_act = self.target_dqn(obs1).detach().max(1)\n",
    "        val1 = self.dqn(obs1).detach()\n",
    "        val1 = val1.gather(1, max_act.view(-1, 1)).squeeze()\n",
    "        \n",
    "        targets = rew + gamma*torch.mul(val1, (1 - dones))\n",
    "        \n",
    "        self.optim.zero_grad()\n",
    "        loss = nn.MSELoss()(val, targets)                \n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "                \n",
    "    def step(self, obs_t1, rew_t, done_t):\n",
    "        \n",
    "        self.replay_buffer.add(self.obs_t, self.act_t, obs_t1, rew_t, done_t)\n",
    "        self.obs_t = obs_t1\n",
    "        self.t = self.t + 1        \n",
    "        \n",
    "        if self.t > self.batch_size and self.t%self.train_freq == 0:\n",
    "            self.train()\n",
    "            \n",
    "        if self.t > self.update_freq and self.t%self.update_freq == 0:\n",
    "            self.update_target()\n",
    "        \n",
    "        self.act_t = self.act(obs_t1)\n",
    "        return self.act_t\n",
    "        \n",
    "    def act(self, obs):\n",
    "        random_prob = np.random.binomial(1, self.epsilon.value(t))\n",
    "        \n",
    "        if random_prob == 1 or not obs.tolist() :\n",
    "            \n",
    "            # Act randomly with epsilon probability\n",
    "            curr_act = self.act_space.sample()\n",
    "            \n",
    "            if self.t%100 == 0:\n",
    "                print(\"Exploring with prob \" + str(self.epsilon.value(t)))\n",
    "                                \n",
    "        else:                           \n",
    "            curr_act = np.argmax(self.dqn(torch.from_numpy(obs).float().unsqueeze(0)).detach().numpy())                        \n",
    "            \n",
    "        return curr_act\n",
    "    \n",
    "    def reset(self, obs):\n",
    "        \n",
    "        self.obs_t = obs\n",
    "        self.act_t = self.act(obs)        \n",
    "        return self.act(obs)\n",
    "        \n",
    "    def update_target(self):\n",
    "        self.target_dqn.load_state_dict(self.dqn.state_dict())     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 30000\n",
    "epsilon = LinearSchedule(steps, 0.05, 1.0)\n",
    "lr = 2e-4\n",
    "batch_size = 32\n",
    "gamma = 1\n",
    "\n",
    "dqn = DQN()\n",
    "dqn.apply(init_weights)\n",
    "agent = DQN_learn(dqn, lr, epsilon, gamma, batch_size, env.action_space)\n",
    "\n",
    "done = True\n",
    "\n",
    "episode_rew = 0\n",
    "episode_count = 0\n",
    "\n",
    "for t in range(steps):\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        act = agent.reset(obs)                    \n",
    "        print(\"Steps = \" + str(t) + \", Episode \" + str(episode_count) + \" with reward = \" + str(episode_rew))\n",
    "        \n",
    "        episode_rew = 0\n",
    "        episode_count = episode_count + 1\n",
    "        \n",
    "    obs, rew, done, _ = env.step(act)   \n",
    "    if done:        \n",
    "        rew = 0\n",
    "    act = agent.step(obs, rew, done)\n",
    "    episode_rew = episode_rew + rew\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steps = 1000\n",
    "agent.epsilon = LinearSchedule(steps, 0.01, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 with reward = 0\n",
      "Episode 1 with reward = 200.0\n",
      "Episode 2 with reward = 200.0\n",
      "Episode 3 with reward = 200.0\n",
      "Episode 4 with reward = 200.0\n",
      "Episode 5 with reward = 200.0\n",
      "Episode 6 with reward = 200.0\n",
      "Exploring with prob 0.01\n",
      "Episode 7 with reward = 200.0\n",
      "Episode 8 with reward = 200.0\n",
      "Episode 9 with reward = 200.0\n",
      "Episode 10 with reward = 200.0\n",
      "Episode 11 with reward = 200.0\n",
      "Episode 12 with reward = 200.0\n",
      "Episode 13 with reward = 200.0\n",
      "Episode 14 with reward = 200.0\n",
      "Episode 15 with reward = 200.0\n",
      "Episode 16 with reward = 200.0\n",
      "Episode 17 with reward = 200.0\n",
      "Episode 18 with reward = 200.0\n",
      "Episode 19 with reward = 200.0\n",
      "Episode 20 with reward = 200.0\n",
      "Episode 21 with reward = 200.0\n",
      "Episode 22 with reward = 200.0\n",
      "Episode 23 with reward = 200.0\n",
      "Episode 24 with reward = 200.0\n",
      "Episode 25 with reward = 200.0\n",
      "Episode 26 with reward = 200.0\n",
      "Episode 27 with reward = 200.0\n",
      "Episode 28 with reward = 200.0\n",
      "Episode 29 with reward = 200.0\n",
      "Episode 30 with reward = 200.0\n",
      "Episode 31 with reward = 200.0\n",
      "Episode 32 with reward = 200.0\n",
      "Episode 33 with reward = 200.0\n",
      "Episode 34 with reward = 200.0\n",
      "Episode 35 with reward = 200.0\n",
      "Episode 36 with reward = 200.0\n",
      "Episode 37 with reward = 200.0\n",
      "Episode 38 with reward = 200.0\n",
      "Episode 39 with reward = 200.0\n",
      "Episode 40 with reward = 200.0\n",
      "Episode 41 with reward = 200.0\n",
      "Episode 42 with reward = 200.0\n",
      "Episode 43 with reward = 200.0\n",
      "Episode 44 with reward = 200.0\n",
      "Episode 45 with reward = 200.0\n",
      "Episode 46 with reward = 200.0\n",
      "Episode 47 with reward = 200.0\n",
      "Episode 48 with reward = 200.0\n",
      "Episode 49 with reward = 200.0\n",
      "Episode 50 with reward = 200.0\n",
      "Episode 51 with reward = 200.0\n",
      "Episode 52 with reward = 200.0\n",
      "Episode 53 with reward = 200.0\n",
      "Episode 54 with reward = 200.0\n",
      "Episode 55 with reward = 200.0\n",
      "Episode 56 with reward = 200.0\n",
      "Episode 57 with reward = 200.0\n",
      "Episode 58 with reward = 200.0\n",
      "Episode 59 with reward = 200.0\n",
      "Episode 60 with reward = 200.0\n",
      "Episode 61 with reward = 200.0\n",
      "Episode 62 with reward = 200.0\n",
      "Exploring with prob 0.01\n",
      "Episode 63 with reward = 200.0\n",
      "Episode 64 with reward = 200.0\n",
      "Episode 65 with reward = 200.0\n",
      "Episode 66 with reward = 200.0\n",
      "Episode 67 with reward = 200.0\n",
      "Episode 68 with reward = 200.0\n",
      "Episode 69 with reward = 200.0\n",
      "Episode 70 with reward = 200.0\n",
      "Episode 71 with reward = 200.0\n",
      "Episode 72 with reward = 200.0\n",
      "Episode 73 with reward = 200.0\n",
      "Episode 74 with reward = 200.0\n",
      "Episode 75 with reward = 200.0\n",
      "Episode 76 with reward = 200.0\n",
      "Episode 77 with reward = 200.0\n",
      "Episode 78 with reward = 200.0\n",
      "Episode 79 with reward = 200.0\n",
      "Episode 80 with reward = 200.0\n",
      "Episode 81 with reward = 200.0\n",
      "Episode 82 with reward = 200.0\n",
      "Episode 83 with reward = 200.0\n",
      "Episode 84 with reward = 200.0\n",
      "Episode 85 with reward = 200.0\n",
      "Episode 86 with reward = 200.0\n",
      "Episode 87 with reward = 200.0\n",
      "Episode 88 with reward = 200.0\n",
      "Episode 89 with reward = 200.0\n",
      "Episode 90 with reward = 200.0\n",
      "Episode 91 with reward = 200.0\n",
      "Episode 92 with reward = 200.0\n",
      "Episode 93 with reward = 200.0\n",
      "Episode 94 with reward = 200.0\n",
      "Episode 95 with reward = 200.0\n",
      "Episode 96 with reward = 200.0\n",
      "Episode 97 with reward = 200.0\n",
      "Exploring with prob 0.01\n",
      "Episode 98 with reward = 200.0\n",
      "Episode 99 with reward = 200.0\n",
      "Episode 100 with reward = 200.0\n"
     ]
    }
   ],
   "source": [
    "#Testing Agent\n",
    "\n",
    "done = True\n",
    "\n",
    "episode_rew = 0\n",
    "episode_count = 0\n",
    "res = []\n",
    "\n",
    "while episode_count <= 100:\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        act = agent.reset(obs)            \n",
    "        \n",
    "        print(\"Episode \" + str(episode_count) + \" with reward = \" + str(episode_rew))\n",
    "        res.append(episode_rew)\n",
    "        episode_rew = 0\n",
    "        episode_count = episode_count + 1\n",
    "        \n",
    "    obs, rew, done, _ = env.step(act)       \n",
    "    act = agent.step(obs, rew, done)\n",
    "    episode_rew = episode_rew + rew\n",
    "\n",
    "#     env.render()\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(res[1:]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Result for 400th episode:__\n",
    "![last_episode](img/cartpole4.gif \"Cartpole\")\n",
    "__Result for 500th episode:__\n",
    "![last_episode](img/cartpole5.gif \"Cartpole\")\n",
    "__Result after 30,000 steps:__\n",
    "![last_episode](img/cartpole_final.gif \"Cartpole\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
